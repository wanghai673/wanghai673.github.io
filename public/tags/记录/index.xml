<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>记录 on Wanghai673 | 博客</title>
    <link>http://localhost:1313/tags/%E8%AE%B0%E5%BD%95/</link>
    <description>Recent content in 记录 on Wanghai673 | 博客</description>
    <image>
      <title>Wanghai673 | 博客</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Tue, 07 Oct 2025 19:59:05 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E8%AE%B0%E5%BD%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>10.9 记录</title>
      <link>http://localhost:1313/posts/10.9_%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Tue, 07 Oct 2025 19:59:05 +0800</pubDate>
      <guid>http://localhost:1313/posts/10.9_%E8%AE%B0%E5%BD%95/</guid>
      <description>&lt;h3 id=&#34;transformer架构学习&#34;&gt;Transformer架构学习&lt;/h3&gt;
&lt;p&gt;学习了self-attention、multi-head attention机制，位置编码等，并手动搭建了Transformer架构。并利用Transfomer框架实现了一个&lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models/ungradedLab/RdNV9/transformer-network-application-named-entity-recognition/lab&#34;&gt;名字实体识别&lt;/a&gt;、&lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models/ungradedLab/6iTj6/transformer-network-application-question-answering/lab?path=%2Fnotebooks%2FW4A3_UGL%2FQA_dataset.ipynb&#34;&gt;问答生成&lt;/a&gt;的任务（模型参数没上去，表现不如RNN）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;3319e3d6922a2e7f2499a3130d3b5925&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/10.9_%E8%AE%B0%E5%BD%95/3319e3d6922a2e7f2499a3130d3b5925.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;llm历史了解&#34;&gt;LLM历史了解&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1、GPT, GPT-2, GPT-3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;GPT，GPT-2，GPT-3 论文精读【论文精读】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT：Transformer的Decoder Only的模型始祖&lt;/li&gt;
&lt;li&gt;GPT-2：加大参数量，并在fewshot领域实现进步，但较同时期Bert进步不明显&lt;/li&gt;
&lt;li&gt;GPT-3：继续加大参数和数据量，模型能力在微调和fewshot后大大提升&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2、Bert&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=2205a224ef8d2cd2b3b9cb444289192a&#34;&gt;BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Encoder Only的模型始祖&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、Instruct GPT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1hd4y187CR/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;InstructGPT 论文精读【论文精读·48】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RLHF 就是这里来的&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4、GPT-4&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vM4y1U7b5/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;GPT-4论文精读【论文精读·53】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;模型能力涌现&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5、Llama 3.1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经典的开源大模型&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1WM4m1y7Uh/?spm_id_from=333.788.videopod.sections&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;Llama 3.1论文精读 · 1. 导言【论文精读·54】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;大模型sft&#34;&gt;大模型SFT&lt;/h3&gt;
&lt;p&gt;雷智凯同学总结的文档来学习大模型的SFT微调，一步步的教学transformer库的核心函数是怎么用的，如tokenizer，AutoModelForCausalLM等；手动搭建Dataset。&lt;/p&gt;
&lt;h4 id=&#34;练习对病句改错数据集进行sft微调&#34;&gt;&lt;strong&gt;练习&lt;/strong&gt;：对病句改错数据集进行SFT微调&lt;/h4&gt;
&lt;h3 id=&#34;-few-shot-示例&#34;&gt;📘 Few-shot 示例&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;用户输入 (user)&lt;/th&gt;
          &lt;th&gt;模型输出 (assistant)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;全国光伏发电平均利用率达98%，利用水平明显提高。&lt;/td&gt;
          &lt;td&gt;全国光伏发电平均利用率达98%，利用水平明显提高。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;#晚安.spuer#希望我们都能在山川尔尔里找到让自己感到快乐还有意义的事，并坚持下去。&lt;/td&gt;
          &lt;td&gt;#晚安.spuer#希望我们都能在山川尔里找到让自己感到快乐还有意义的事，并坚持下去。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;按照定逾期未检验车辆不得上路行驶。&lt;/td&gt;
          &lt;td&gt;按照规定逾期未检验车辆不得上路行驶。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✨你终回像星星那般发光发亮早安#早安#&lt;/td&gt;
          &lt;td&gt;✨你总会像星星那般发光发亮早安#早安#&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-llm-finetune-实验结果表&#34;&gt;🔬 LLM Finetune 实验结果表&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;ID&lt;/th&gt;
          &lt;th&gt;System Prompt&lt;/th&gt;
          &lt;th&gt;Finetune&lt;/th&gt;
          &lt;th&gt;Few-shot&lt;/th&gt;
          &lt;th&gt;Avg. Acc (%)&lt;/th&gt;
          &lt;th&gt;Pos. Acc (%)&lt;/th&gt;
          &lt;th&gt;Neg. Acc (%)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;56.02&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;82.4&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;30.35&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;53.55&lt;/td&gt;
          &lt;td&gt;71.4&lt;/td&gt;
          &lt;td&gt;36.19&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;9.76&lt;/td&gt;
          &lt;td&gt;16.4&lt;/td&gt;
          &lt;td&gt;3.31&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;7.30&lt;/td&gt;
          &lt;td&gt;12.20&lt;/td&gt;
          &lt;td&gt;2.53&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;3.55&lt;/td&gt;
          &lt;td&gt;5.2&lt;/td&gt;
          &lt;td&gt;1.95&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-总结观察&#34;&gt;💡 总结观察&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;System Prompt + Finetune + Few-shot (ID 6)&lt;/strong&gt; 组合效果最佳，平均准确率最高（56.02%），正样本识别表现尤其突出（82.4%）。&lt;/li&gt;
&lt;li&gt;仅使用 &lt;strong&gt;System Prompt + Finetune (ID 4)&lt;/strong&gt; 也表现良好，但略低于全组合。&lt;/li&gt;
&lt;li&gt;缺少 Finetune 或 Few-shot 时（如 ID 5、2），性能急剧下降。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无任何增强 (ID 1)&lt;/strong&gt; 表现最差，验证了各增强手段的重要性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;id-6-微调checkpoint下变化&#34;&gt;ID 6 微调checkpoint下变化&lt;/h4&gt;
&lt;p&gt;![output (4)](output (4).png)&lt;/p&gt;</description>
    </item>
    <item>
      <title>10.9 记录</title>
      <link>http://localhost:1313/posts/%E7%BB%84%E4%BC%9A%E5%B1%95%E7%A4%BA/</link>
      <pubDate>Tue, 07 Oct 2025 19:59:05 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E7%BB%84%E4%BC%9A%E5%B1%95%E7%A4%BA/</guid>
      <description>&lt;h3 id=&#34;transformer-架构学习&#34;&gt;Transformer 架构学习&lt;/h3&gt;
&lt;p&gt;学习了 self-attention、multi-head attention 机制，位置编码等，并手动搭建了 Transformer 架构。并利用 Transfomer 框架实现了一个&lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models/ungradedLab/RdNV9/transformer-network-application-named-entity-recognition/lab&#34;&gt;名字实体识别&lt;/a&gt;、&lt;a href=&#34;https://www.coursera.org/learn/nlp-sequence-models/ungradedLab/6iTj6/transformer-network-application-question-answering/lab?path=%2Fnotebooks%2FW4A3_UGL%2FQA_dataset.ipynb&#34;&gt;问答生成&lt;/a&gt;的任务（模型参数没上去，表现不如 RNN）。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;3319e3d6922a2e7f2499a3130d3b5925&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E7%BB%84%E4%BC%9A%E5%B1%95%E7%A4%BA/3319e3d6922a2e7f2499a3130d3b5925.png&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;llm-历史了解&#34;&gt;LLM 历史了解&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1、GPT, GPT-2, GPT-3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;GPT，GPT-2，GPT-3 论文精读【论文精读】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT：Transformer 的 Decoder Only 的模型始祖&lt;/li&gt;
&lt;li&gt;GPT-2：加大参数量，并在 fewshot 领域实现进步，但较同时期 Bert 进步不明显&lt;/li&gt;
&lt;li&gt;GPT-3：继续加大参数和数据量，模型能力在微调和 fewshot 后大大提升&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2、Bert&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=2205a224ef8d2cd2b3b9cb444289192a&#34;&gt;BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Encoder Only 的模型始祖&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3、Instruct GPT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1hd4y187CR/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;InstructGPT 论文精读【论文精读·48】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RLHF 就是这里来的&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4、GPT-4&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1vM4y1U7b5/?spm_id_from=333.1387.search.video_card.click&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;GPT-4 论文精读【论文精读·53】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;模型能力涌现&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5、Llama 3.1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;经典的开源大模型&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1WM4m1y7Uh/?spm_id_from=333.788.videopod.sections&amp;amp;vd_source=cf35d5107dda9df709c41cc1ec25735f&#34;&gt;Llama 3.1 论文精读 · 1. 导言【论文精读·54】_哔哩哔哩_bilibili&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;大模型-sft&#34;&gt;大模型 SFT&lt;/h3&gt;
&lt;p&gt;雷智凯同学总结的文档来学习大模型的 SFT 微调，一步步的教学 transformer 库的核心函数是怎么用的，如 tokenizer，AutoModelForCausalLM 等；手动搭建 Dataset。&lt;/p&gt;
&lt;h4 id=&#34;练习对病句改错数据集进行-sft-微调&#34;&gt;&lt;strong&gt;练习&lt;/strong&gt;：对病句改错数据集进行 SFT 微调&lt;/h4&gt;
&lt;h3 id=&#34;-few-shot-示例&#34;&gt;📘 Few-shot 示例&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;用户输入 (user)&lt;/th&gt;
          &lt;th&gt;模型输出 (assistant)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;全国光伏发电平均利用率达 98%，利用水平明显提高。&lt;/td&gt;
          &lt;td&gt;全国光伏发电平均利用率达 98%，利用水平明显提高。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;#晚安.spuer#希望我们都能在山川尔尔里找到让自己感到快乐还有意义的事，并坚持下去。&lt;/td&gt;
          &lt;td&gt;#晚安.spuer#希望我们都能在山川尔里找到让自己感到快乐还有意义的事，并坚持下去。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;按照定逾期未检验车辆不得上路行驶。&lt;/td&gt;
          &lt;td&gt;按照规定逾期未检验车辆不得上路行驶。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;✨ 你终回像星星那般发光发亮早安#早安#&lt;/td&gt;
          &lt;td&gt;✨ 你总会像星星那般发光发亮早安#早安#&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-llm-finetune-实验结果表&#34;&gt;🔬 LLM Finetune 实验结果表&lt;/h3&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;ID&lt;/th&gt;
          &lt;th&gt;System Prompt&lt;/th&gt;
          &lt;th&gt;Finetune&lt;/th&gt;
          &lt;th&gt;Few-shot&lt;/th&gt;
          &lt;th&gt;Avg. Acc (%)&lt;/th&gt;
          &lt;th&gt;Pos. Acc (%)&lt;/th&gt;
          &lt;th&gt;Neg. Acc (%)&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;56.02&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;82.4&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;30.35&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;53.55&lt;/td&gt;
          &lt;td&gt;71.4&lt;/td&gt;
          &lt;td&gt;36.19&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;9.76&lt;/td&gt;
          &lt;td&gt;16.4&lt;/td&gt;
          &lt;td&gt;3.31&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;7.30&lt;/td&gt;
          &lt;td&gt;12.20&lt;/td&gt;
          &lt;td&gt;2.53&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;✅&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;3.55&lt;/td&gt;
          &lt;td&gt;5.2&lt;/td&gt;
          &lt;td&gt;1.95&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;❌&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
          &lt;td&gt;0&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-总结观察&#34;&gt;💡 总结观察&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;System Prompt + Finetune + Few-shot (ID 6)&lt;/strong&gt; 组合效果最佳，平均准确率最高（56.02%），正样本识别表现尤其突出（82.4%）。&lt;/li&gt;
&lt;li&gt;仅使用 &lt;strong&gt;System Prompt + Finetune (ID 4)&lt;/strong&gt; 也表现良好，但略低于全组合。&lt;/li&gt;
&lt;li&gt;缺少 Finetune 或 Few-shot 时（如 ID 5、2），性能急剧下降。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;无任何增强 (ID 1)&lt;/strong&gt; 表现最差，验证了各增强手段的重要性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;id-6-微调-checkpoint-下变化&#34;&gt;ID 6 微调 checkpoint 下变化&lt;/h4&gt;
&lt;p&gt;![output (4)](output (4).png)&lt;/p&gt;</description>
    </item>
    <item>
      <title>9.24 记录</title>
      <link>http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Wed, 24 Sep 2025 16:10:02 +0800</pubDate>
      <guid>http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/</guid>
      <description>&lt;p&gt;Coursera上的深度学习课程算是完成了个大概，序列模型里还是学到很多实用的知识，如RNN、GRU、LSTM、Embedding、Attention机制、transformer模型。&lt;/p&gt;
&lt;p&gt;下面是&lt;strong&gt;原始RNN&lt;/strong&gt;的一些知识点和发展历史：&lt;/p&gt;
&lt;h4 id=&#34;rnn的不同应用&#34;&gt;RNN的不同应用&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;语音识别&lt;/li&gt;
&lt;li&gt;音乐生成&lt;/li&gt;
&lt;li&gt;情感分类&lt;/li&gt;
&lt;li&gt;DNA序列分析&lt;/li&gt;
&lt;li&gt;机器翻译&lt;/li&gt;
&lt;li&gt;视频活动识别&lt;/li&gt;
&lt;li&gt;命名实体识别&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;rnn的架构类型&#34;&gt;RNN的架构类型&lt;/h4&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924203943627&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924203943627.png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;rnn的生成语言模型-和-采样&#34;&gt;RNN的生成语言模型 和 采样&lt;/h4&gt;
&lt;p&gt;学习beamsearch算法对模型生产序列进行采样，本质上也是种贪心算法。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924204330436&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924204330436.png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;引入lstm和gru&#34;&gt;引入LSTM和GRU&lt;/h4&gt;
&lt;p&gt;传统RNN随着上下文一长，由于模型隐藏层变量大小限制，会丢失掉部分上下文信息。随后，引入了记忆门的装置，能够使得记忆信息直接传递给后续节点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GRU结构：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924204739897&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924204739897.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LSTM结构：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924204817688&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924204817688.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于特定问题，如完形填空等，需要不仅前文内容，还需要后文的内容随后，引入&lt;strong&gt;双向RNN&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924205008239&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924205008239.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;一个全连接层可能，无法很好提取上下文信息，随后引入&lt;strong&gt;深度RNN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924205134272&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924205134272.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;之后完成了JAZZ创造的&lt;strong&gt;生成式RNN的作业&lt;/strong&gt;：https://www.coursera.org/learn/nlp-sequence-models/programming/ZS7X2/jazz-improvisation-with-lstm/lab&lt;/p&gt;
&lt;p&gt;课程内容都浅显易懂，并且配有作业。在上面的学习，主要深刻意识到了DL是一个很厉害的工具，我们可以用他做很多有意义的事情，并服务人类。而且并没有什么应用难度，人人都可以学习DL这个工具，应用到自己生活中（虽然黑盒，也就因为黑盒，才能超越人类认知）&amp;hellip;&lt;/p&gt;
&lt;p&gt;后面主要继续深入LLM的学习中，多看一些LLM经典论文，先打好基础，再投入到前沿研究中。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250924161123376&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.24_%E8%AE%B0%E5%BD%95/image-20250924161123376.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>9.15 记录</title>
      <link>http://localhost:1313/posts/9.15_%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Mon, 15 Sep 2025 22:06:20 +0800</pubDate>
      <guid>http://localhost:1313/posts/9.15_%E8%AE%B0%E5%BD%95/</guid>
      <description>&lt;p&gt;今天主要看完了Coursera的CNN的week 2，主要是介绍了几个CNN里常见的网络，如classic CNN、Resnet、inception、mobile network，EfficientNet（mobile的微改版本）。&lt;/p&gt;
&lt;p&gt;之后介绍了CNN项目的一些实践经验：如要开源、迁移学习、数据增强。&lt;/p&gt;
&lt;p&gt;主要收获是重学了下Resnet，意识到short cut可以连跨过好几层网络连接（之前还以为只能跨越一层）&amp;hellip;&lt;/p&gt;
&lt;p&gt;最后完成了Resnet和迁移学习的tensorflow代码实现（简单填空），实现了resnet-50（但我训练过拟合这么快？），有了深度学习框架就是方便~&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250915221235620&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.15_%E8%AE%B0%E5%BD%95/image-20250915221235620.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;论文只看了个摘要，还是保持下论文阅读的习惯吧，毕竟读论文还挺有趣的（读多了感觉我才能侃侃而谈hhh）&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
