<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>论文阅读 on Wanghai673 | 博客</title>
    <link>http://localhost:1313/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
    <description>Recent content in 论文阅读 on Wanghai673 | 博客</description>
    <image>
      <title>Wanghai673 | 博客</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Sun, 07 Sep 2025 10:58:46 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>9.7 论文阅读</title>
      <link>http://localhost:1313/posts/9.7_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sun, 07 Sep 2025 10:58:46 +0800</pubDate>
      <guid>http://localhost:1313/posts/9.7_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>&lt;h3 id=&#34;self-collaboration-code-generation-via-chatgpt&#34;&gt;Self-collaboration Code Generation via ChatGPT&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2023年的一篇上古论文了（GPT-3.5时代），讲的是如何通过多智能体协作结合软件工程的一些方法论实现代码生成。
框架中不同智能体是通过prompt驱动的。&lt;/p&gt;
&lt;p&gt;首先将问题分解为不同stage，如何不同stage通过三个智能体：分析师、代码师、测试师 来解决完成的，如下图所示。&lt;/p&gt;
&lt;p&gt;然后作者针对框架提出6个不同的RQ(论文标题写错了RQ6写成RQ7了hhh)，并实验分析。&lt;/p&gt;
&lt;p&gt;RQ1: Self-collaboration vs. Baselines&lt;/p&gt;
&lt;p&gt;RQ2: The Effect of Roles in Self-collaboration&lt;/p&gt;
&lt;p&gt;RQ3: Self-collaboration on Different LLMs&lt;/p&gt;
&lt;p&gt;RQ4: The Effect of Interaction&lt;/p&gt;
&lt;p&gt;RQ5: Analysis for Self-collaboration&lt;/p&gt;
&lt;p&gt;RQ6: How does self-collaboration work in repository-level software development scenarios and how does it perform?&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250907110022476&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.7_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20250907110022476.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>9.6 论文阅读</title>
      <link>http://localhost:1313/posts/9.6_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Sat, 06 Sep 2025 12:25:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/9.6_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>&lt;h3 id=&#34;eduagent-generative-student-agents-in-learning&#34;&gt;EduAgent: Generative Student Agents in Learning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文是针对线上教育领域的学生模仿相关的研究，之前的模型多利用庞大的数据对学生学习行为进行预测，随着LLM的问世，LLM提供的前置知识能很好的针对不同场景不同内容线上教育的学生行为预测。&lt;/p&gt;
&lt;p&gt;学生行为预测受到多方面的影响，如性格、学生储备知识等，本文提供了一个数据集（350个sample），针对一段5分钟的幻灯片讲解，提供学生个人信息和每个小时间段的学生注意窗口、行为、认知状态信息等。&lt;/p&gt;
&lt;p&gt;作者结合LLM强大的推理功能，让LLM自主推理出不同信息间的关联，从而实现学生行为的预测和模仿，但这篇论文的实验没咋看懂&amp;hellip;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;llm-mediated-domain-specific-voice-agents-the-case-of-textilebot&#34;&gt;LLM-mediated domain-specific voice agents: the case of TextileBot&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;粗粒度地看了下&amp;hellip;&lt;/p&gt;
&lt;p&gt;只看了摘要和引言和结论，讲的是如何原型化地设计一个垂直领域的对话agent，包含prompt模板，随后作者自己设计了一个宣传服装环保领域的一个agent（在购物的时候跟顾客交流的），并做了用户实验。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;why-language-models-hallucinate&#34;&gt;Why language models hallucinate&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://openai.com/index/why-language-models-hallucinate/&#34;&gt;Why language models hallucinate | OpenAI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenAI 9月5号刚发布的文章，主要讲述了为什么大语言模型会产生幻觉。&lt;/p&gt;
&lt;p&gt;作者描述，LLM（大语言模型）之所以产生幻觉，是因为现在对模型后训练的奖励函数，往往将答错一道题和拒答一道题（承认不知道）的惩罚都是一致的，导致LLM更倾向于去猜题，这样还有概率猜对。&lt;/p&gt;
&lt;p&gt;那有人就会说了，让答错的惩罚提升一些不就行了。确实可以，但是作者回应到现如今大多数的benchmark，只有对/错两个选项，并没有考虑到幻觉的因素，从而导致大家更宁愿LLM猜题，增加一些benchmark的准确率，而不是拒答（大幅度降低幻觉，但准确率会略微降低）。&lt;/p&gt;
&lt;p&gt;作者呼吁所有benchmark的制作者们，将幻觉这个评价指标加入到benchmark的评测之中，从而抑制LLM的胡言乱语。但现在的benchmark对幻觉的评判往往是特定的一类，大多数benmark没有考虑幻觉因素。&lt;/p&gt;
&lt;p&gt;上面都是通过后训练降低LLM幻觉的途径，那能不能在预训练的时候降低大语言模型的幻觉呢？作者回答，现在的数据都是无监督的，导致LLM并没有办法对每段数据的真实性做判断，更好的办法还是在后训练的时候减少大模型的幻觉。&lt;/p&gt;
&lt;p&gt;下图是原文的提出的LLM幻觉的一些误解与澄清：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250906164329093&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9.6_%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20250906164329093.png&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>9.5 论文阅读</title>
      <link>http://localhost:1313/posts/9%E6%9C%885%E6%97%A5%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</link>
      <pubDate>Fri, 05 Sep 2025 13:35:14 +0800</pubDate>
      <guid>http://localhost:1313/posts/9%E6%9C%885%E6%97%A5%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</guid>
      <description>&lt;h3 id=&#34;llm-agents-for-education-advances-and-applications&#34;&gt;LLM Agents for Education: Advances and Applications&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这是一篇综述论文，讲述了现在LLM agents在领域的各种运用。作者将教育agent分为 教学agent 和 垂直领域agent。&lt;/p&gt;
&lt;p&gt;教学agent又分为面向学生和面向教师的，垂直领域的分为很多不同学科类的agent，不同学科agent也有不同的功能的agent，整体分类思维导图见下图。&lt;/p&gt;
&lt;p&gt;然后作者讲述了，现有教育agent所面临的困难，如道德、偏见方面的，也有幻觉、可靠性方面的。现有的agent大多数不具备结构化的形式（更多是对话类型），无法直接在现实教育场景中插入。&lt;/p&gt;
&lt;p&gt;最后整理介绍了现有测评教育agent各种benchmark，以供后来研究者使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250905134524279&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9%E6%9C%885%E6%97%A5%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20250905134524279.png&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;mathagent-leveraging-a-mixture-of-math-agent-framework-for-real-world-multimodal-mathematical-error-detection&#34;&gt;MATHAGENT: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;大语言模型在数学问题求解中已经非常厉害了，但是在数学错误发现上还很少涉足。传统的数学老师批改，耗时耗力，并且不具备可扩展性，如无法为学生指明学生具体错误（人工成本高），多模态大模型可以识别图片，并且为学生提供错误位置和错误分类，但是MLLM还在存在较多错误，对于有细微错误的地方无法很好识别。并且现在研究多聚焦纯文本数学题改错，对于带有图片信息的不能很好应对。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;数据集合为下图，有文本问题描述，图片类信息，正确答案，学生的不正确答案，解题步骤。
任务分为  1：找到第一个错误步骤，2：错误分类，指标都是准确率。&lt;/p&gt;
&lt;img src=&#34;image-20250905153558820.png&#34; alt=&#34;image-20250905153558820&#34; style=&#34;zoom:67%;&#34; /&gt;
&lt;p&gt;作者引入了个多智能体框架，分为三个部分，流程如下图。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本-图像一致性检测：判断图片和文字是否高度一致，避免使用图片识别功能，从而增强题目解读能力&lt;/li&gt;
&lt;li&gt;公式-表格识别：用专门model将各种公式、表格等识别为文本形式&lt;/li&gt;
&lt;li&gt;融合找错：将文本和转文本的图片信息融合为一个完整题目，并且让LLM完成上面具体两个任务&lt;img src=&#34;image-20250905153953903.png&#34; alt=&#34;image-20250905153953903&#34; style=&#34;zoom:67%;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;实验结果&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将该方法运用到多个多模态大模型，大部分都能提升准确率，但相较于人工还有很大的差距。
&lt;img alt=&#34;image-20250905154103167&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/9%E6%9C%885%E6%97%A5%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/image-20250905154103167.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;论文写的浅显易懂，结构也很清晰，可惜本论文并未公布数据集。。。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
