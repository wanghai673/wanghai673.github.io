<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Wanghai673 | 博客</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Wanghai673 | 博客</description>
    <image>
      <title>Wanghai673 | 博客</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.148.2</generator>
    <language>en</language>
    <lastBuildDate>Mon, 25 Aug 2025 15:43:11 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>最近在做的一些事</title>
      <link>http://localhost:1313/posts/%E6%9C%80%E8%BF%91%E5%9C%A8%E5%81%9A%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/</link>
      <pubDate>Mon, 25 Aug 2025 15:43:11 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%9C%80%E8%BF%91%E5%9C%A8%E5%81%9A%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/</guid>
      <description>&lt;p&gt;最近都在广泛学习一些有趣的内容，主要是通过coursera平台。在家呆了2个多月了，也不知道未来啥方向，目前也是有些迷茫的状态，还是学些东西充实下自己吧，搜寻一下感兴趣的东东。&lt;/p&gt;
&lt;p&gt;coursera平台类似于国外的mooc，国内外还有好多这样的平台，如中国大学mooc、bilibili大学、网易云课等等。coursera需要付费会员才能观看课程，而且只能通过国外信用卡付款（无）。于是，我在淘宝一搜，这么多卖号，而且很便宜，86半年，立马充值了一波。&lt;/p&gt;
&lt;p&gt;有了平台，我立马重新在coursera里，拾起深度学习的基础知识，吴恩达的课程。之前只听过，在coursera认真地观看，还是感觉受益匪浅，毕竟有作业有互动，感觉b站还是差点味道。&lt;/p&gt;
&lt;p&gt;现在主要看了深度学习专项课程的前3个：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250825155253827&#34; loading=&#34;lazy&#34; src=&#34;.%5Cimage-20250825155253827.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;其实就相当于复习了一下之前在李沐动手学深度学习的内容。&lt;/p&gt;
&lt;p&gt;然后光看深度学习也有些乏味，我在网上搜索coursera有没有其他好课程，打开了一个课程排名，类似于垃圾小网站的那种：&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;image-20250825155452140&#34; loading=&#34;lazy&#34; src=&#34;.%5Cimage-20250825155452140.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;然后最近在看幸福科学，才刚开始看，讲的是如何变得幸福，通过科学的方式，感觉讲的还挺好的，在实验的加持下，科学成为人们可以坚持最大的迷信，hhh，但是还是要有反驳精神，感觉很多科学实验严谨性还是有些欠缺。&lt;/p&gt;
&lt;p&gt;之后的日子，想了解一下金融相关的知识，挺感兴趣的。马上要开学了，5555，希望能保持学习一些课外内容的习惯。还有，运动也要保持，在家和爸妈打了一个暑假的乒乓球，每日锻炼~&lt;/p&gt;</description>
    </item>
    <item>
      <title>Softmax的反向传播推导</title>
      <link>http://localhost:1313/posts/softmax%E7%9A%84%E5%AF%BC%E6%95%B0%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Thu, 21 Aug 2025 17:07:30 +0800</pubDate>
      <guid>http://localhost:1313/posts/softmax%E7%9A%84%E5%AF%BC%E6%95%B0%E6%8E%A8%E5%AF%BC/</guid>
      <description>&lt;p&gt;softmax是一个经典的多分类概率分布的公式，有n个类，softmax公式如下，
&lt;/p&gt;
$$
\text{softmax}(z)_j = \frac{e^{z_j}}{\sum_{i=1}^n  e^{z_i}}, \quad j = 1, 2, \dots, n
$$&lt;p&gt;
首先，考虑一个样本每个类的预测分数做一个softmax，转换成每类预测的概率形式。&lt;/p&gt;
$$
a^{[L]} = \text{softmax}(z^{[L]})
$$$$
\hat{y} = a^{[L]}\quad
$$&lt;p&gt;然后，利用样本标签，结合极大似然估计的损失函数如下。
&lt;/p&gt;
$$
L(\hat{y}, y) = - \sum_{i=1}^{n} y_i \log(\hat{y}_i)
$$&lt;p&gt;由于需要做反向传播，我们想求 $\frac{\partial L}{\partial z^{[L]}}$，也就是做完softmax的前的偏导数。&lt;/p&gt;
&lt;p&gt;假设 $y$ 的分类为 $1$，则 $y_1 = 1$，其余 $y_j = 0\ (j\neq 1)$，$y=[1,0,.....,0]$
则有，
&lt;/p&gt;
$$
L(\hat{y}, y) = -\log(\hat{y}_1)
$$$$
L(\hat{y}, y) = - \ln \left( \frac{e^{z_1^{[L]}}}{e^{z_1^{[L]}} + e^{z_2^{[L]}} + \cdots + e^{z_n^{[L]}}} \right),
\quad S = e^{z_1^{[L]}} + e^{z_2^{[L]}} + \cdots + e^{z_n^{[L]}}
$$&lt;p&gt;接下来求 $Z^{[L]}$ 的偏导，
&lt;/p&gt;</description>
    </item>
    <item>
      <title>神经网络反向传播 数学推导</title>
      <link>http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</link>
      <pubDate>Mon, 18 Aug 2025 09:48:24 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/2.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;最近在重新回顾深度学习相关基础，之前大概过了一遍李沐的动手学深度学习，但很多内容还一知半解，
感觉网上课程难度曲线还是不太平滑。&lt;/p&gt;
&lt;p&gt;无意间看到 &lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera平台&lt;/a&gt;，在淘宝上花了80买了个半年号，仿佛打开新世界了，第一次看到了原来真的有课程能把深度学习的学习曲线
弄的这么平滑的，爱了，有点像算法竞赛的acwing、牛客。感觉b站确实不错，但是反馈和互动肯定远远比不上Coursera的。&lt;/p&gt;
&lt;p&gt;现在刚学完第一门课程，并手动设计并实现了多层卷积神经网络（只用到numpy），在这里记录一下forward/back propagation的数学知识（主要是线性代数）。&lt;/p&gt;
&lt;h4 id=&#34;正向传播&#34;&gt;正向传播&lt;/h4&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/%281%29.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;非常简单易懂，就是矩阵乘法。&lt;/p&gt;
&lt;h4 id=&#34;反向传播&#34;&gt;反向传播&lt;/h4&gt;
&lt;h5 id=&#34;引理一&#34;&gt;引理一&lt;/h5&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/%282%29.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/%283%29.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;搞懂这个引理反向传播就差不多弄懂了，求哪个偏导，固定某行/列值算贡献 就好求了（A固定行，B固定列），因为矩阵乘法是B的列与A的行做点积。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/posts/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/%284%29.jpg&#34;&gt;&lt;/p&gt;
&lt;p&gt;以上就是神经网络里的线性代数的一些知识，其实不需要太多基础弄得矩阵乘法，和求导链式法则即可手写神经网络。&lt;/p&gt;</description>
    </item>
    <item>
      <title>hugo&#43;papermod博客搭建教程</title>
      <link>http://localhost:1313/posts/hugo&#43;paermod%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</link>
      <pubDate>Sun, 17 Aug 2025 14:34:53 +0800</pubDate>
      <guid>http://localhost:1313/posts/hugo&#43;paermod%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/</guid>
      <description>&lt;h3 id=&#34;工具介绍&#34;&gt;工具介绍&lt;/h3&gt;
&lt;h4 id=&#34;hugo&#34;&gt;Hugo&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：Hugo 是一个用 Go 语言编写的&lt;strong&gt;静态网站生成器&lt;/strong&gt;。它以极快的生成速度著称，可以在几秒内构建上千页面。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不依赖数据库，所有页面都是静态文件，部署简单且性能高。&lt;/li&gt;
&lt;li&gt;支持 Markdown 写作，适合博客和文档站点。&lt;/li&gt;
&lt;li&gt;拥有强大的模板系统和主题生态。&lt;/li&gt;
&lt;li&gt;跨平台运行（Windows、Linux、macOS）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;papermod&#34;&gt;PaperMod&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;简介&lt;/strong&gt;：PaperMod 是 Hugo 社区里非常流行的一个 &lt;strong&gt;简洁、现代的主题&lt;/strong&gt;，灵感来自 Google Material Design。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;设计简洁清爽，注重阅读体验。&lt;/li&gt;
&lt;li&gt;自带夜间模式、搜索、标签分类等功能。&lt;/li&gt;
&lt;li&gt;对 SEO 和性能友好，开箱即用。&lt;/li&gt;
&lt;li&gt;支持高度自定义，比如文章目录、社交链接、评论系统等。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;👉 总结：&lt;strong&gt;Hugo&lt;/strong&gt; 是建站工具，&lt;strong&gt;PaperMod&lt;/strong&gt; 是在 Hugo 上常用的主题之一。&lt;/p&gt;
&lt;h3 id=&#34;step-1&#34;&gt;Step 1&lt;/h3&gt;
&lt;p&gt;按照下面视频安装hugo+papermod，搭建demo&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1bovfeaEtQ?vd_source=89574dec834a4f547f86ccea8df6514e&#34;&gt;【雷】Hugo + Github免费搭建博客，并实现自动化部署&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-2&#34;&gt;Step 2&lt;/h3&gt;
&lt;p&gt;配置papermod主题，结合大语言模型，想要什么功能直接询问即可，并修改配置文件，主题大部分都是配有相关功能的。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
